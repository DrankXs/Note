# 001
英文：Y-Tuning: An Efficient Tuning Paradigm for Large-Scale Pre-Trained Models via Label Representation Learning
中文：$\mathcal{Y}$-调优：通过标签表示学习的一个有效调优大规模PLM的范式
2023
## 问题和现状
有效的使PLM应用到下游任务引起了巨大关注；
以前的工作主要设计参数高效的调优范式，但需要保存和计算整个计算图的梯度（微调）；
轻量级微调冻结大部分PLM的参数，使用小型可训练模块扩充模型；（适配调优：插入特定于任务的模块训练；提示调优：在前面添加提示向量，训练提示向量实现）
## 创新和方法
提出$\mathcal{Y}$-Tuning，可使冻结的PLM适应特定的下游任务；
它学习标签$\mathcal{Y}$的密集表示，将其对齐到固定的特征表示。

## 实验和结论
对于16亿参数的$DeBERTa_{XLL}$，Y-tuning在GLUE Benchmark只需要2%的可调参数，达到原本微调性能的96%以上。

## 自己想法
Y-tuning的核心在于对输出进行向量化，而不是将原有的向量映射到输出；
由于输出的有限性，所以只需要简单的模型就可以将其区分出来，并很好的表达；
使得输出向量化的部分进行训练，向输入的语义向量逼近，计算两者相似度，就可以判断输入最想要的是那个输出。

# 002
英文：Prompt-Driven Neural Machine Translation
中文：提示驱动的神经翻译模型
2022

## 问题和现状
NMT（神经网络机器翻译）仍缺乏脆弱性和风格灵活性，用于针对实例级的约束的方法往往特定于约束或者模型。

## 创新和方法
提出 prompt驱动的神经机器翻译。加强翻译控制和丰富灵活性。
比较了多种prompt方法，寻找最优将源句和prompt进行编码的最有效系统架构；
设计了一种算法，从标准MT（机器翻译）训练语料库中自动构造不同类型的提示符，训练一个混合提示符的模型；
考虑基于抽样的训练策略，随机选择提示的种类和数量；
将提示驱动部署在一个真实的场景，专业的翻译人员通过提示进行机器翻译后编辑。

## 实验和结论 
数据集：
IWSLT'14 De->En；WMT' 17 En->Zh；
评价指标：BLEU；multi-bleu; sacreBLEU。Response Rate。

## 自己想法
本身对于NMT人物其实并不了解，但是文中将prompt与Transformer结合的方法值得借鉴，除了传统的Prefix和continues prompt，尝试使用了基于交叉注意力在attention子层，这个方法在Decoder进行改造，对所有的生成任务都有效。

# 003
英文：SELF-CONSISTENCY IMPROVES CHAIN OF THOUGHT REASONING IN LANGUAGE MODELS
中文：自一致性提高思维链在语言模型的推理能力
2022
## 问题和现状
思维链在PLM中的复杂推理任务中获得了优秀的成果；
## 创新和方法
提出新的解码策略：自一致性，取代原本思维链使用的贪婪解码方法。
对一组不同的推理路径采样，然后将采样的推理路径边缘化来选择最一致的答案；
自一致性的灵感来于推理时存在多个不同的思维方式;
从LM中采样，以生成多样化的推理路径集，通过边缘抽样的推理路径来确定最优答案，以找到最终答案集中最一致的答案。
## 实验和结论
在流行的推理数据集上，提高了思维链提示的表现：
GSM8K:17.9%； SVAMP：11.0%； AQuA:12.2%; StrategyQA:6.4%； ARC-challenge: 3.9%
## 自己想法
利用PLM得到不同的结果，这点对于NLP任务是很有必要的，这样子避免简单的贪心造成的误差。
采样是一个很好的办法


# 004
英文：Inverse is Better! Fast and Accurate Prompt for Few-shot Slot Tagging
中文：反转更好！针对小样本槽标记快速准确的提示
2022
## 问题和现状
提示学习大量成功；
针对槽标记任务提示学习的表现并没有那么好；
提示方法必须枚举所有n-grams标记跨度才能找到所有可能的槽，降低了预测速度。

## 创新和方法
引入相反范例，与将标记映射到标签的经典prompt不同，反向预测给定槽类型的槽值；
这种逆提示只需要进行一次预测，大大加快了预测速度；
提出了新的迭代预测策略，通过考虑不同插槽类型之间的关系来学习改进预测；
## 实验和结论
在10-shot下，预测更快且显著提高了性能；
## 自己想法
从原来的确定源，选择是什么实体，变成判断某种实体是否是源中的一部分，如果是，是那一部分。
本质还是完形填空式的prompt。

# 005
英文：Summarizing Chinese Medical Answer with Graph Convolution Networks and Question-focused Dual Attention
中文：
2020
## 问题和现状
查询医疗问题使用搜索引擎，为在线搜索引擎生成对应的答案摘要很必要，经常期望能够揭示问题的直接答案；

## 创新和方法
提出了利用图卷积网络和以问题为中心的双重注意力的中医答案汇总方法。
先用图卷积网络将原来的长答案文本组织成一个医学概念图，方便更好理解文本内部结构和医学概念之间的相关性；
之后引入以问题为中心的双重注意机制来生成与问题相关的摘要。

## 实验和结论
与base比，该模型可以生成更连贯，更有信息量的摘要；

## 自己想法 
对于问答，添加问题到答案生成时的交叉注意力是很好的办法；
提前NER识别出对应的医学实体是有必要的，NER的应用是很方便的；


## 方法

**问题定义**
$A$表示包含几个句子$\left[s_{1},\,s_{2},\,s_{3},\,s_{4},\,\cdot\,\cdot\,\cdot\,\,,\,s_{m}\right]$的答案；$s_i$是答案中的第i个句子，Q表示输入问题，目的是生成与Q最相关的A的抽象摘要。

**医学概念图构造**
医学概念定义为作为文本重要组成部分的医学实体或关键字的短语/单词。
1.给定输入问题Q和一个答案A，分词；
2.使用预训练好的BERT-CRF对文本进行医学命名实体识别（NER）
3.使用TextRank获得提取出来的关键词
4.这些关键词为答案的概念集合。
5.将答案中的每个句子与其对应的概念联系起来。（即如果$\omega$出现在句子里，则把句子分配给概念$\omega$）
6.之后，一个句子会连接着多个概念，这就暗含了概念之间的关联。
7.定义不连概念的句子为空节点；
8.句子和概念都定义为定点$v_k$；
9.则句子和概念之间的关系为边。
10.概念节点，如果共享一个句子，添加边，权重为共享句子的数量，即下述$\phi$。
![20230308102049](https://cdn.jsdelivr.net/gh/DrankXs/picrepo/20230308102049.png)


**节点初始化**
每个节点$v_i$用向量$u_i$表示。

vertex encoder：由embedding模块和多头自注意模块构成；
无论是选出的概念token还是普通的规则token，共享一个embedding矩阵，表示词信息，针对每个词得到$\omega_i$为词嵌入向量，并添加绝对位置向量$p_i^{absolute}$和相对位置向量$p_i^{relative}$一同作为嵌入结果：
$u_{i}=w_{i}+p_{i}^{a b s o l u t e}+p_{i}^{r e l a t i v e}$
之后将$u_i$输入到自注意力模块，获取hidden 表示$a_i$;
其中，添加$a_0$表示整个顶点。

**图卷积网络**
定义邻接矩阵 $A\in R^{N\times N}$，$A_{i j}\,=\,w_{i j}$；
$D$是一个对角矩阵；
$I_N$是一个单位矩阵；
$\tilde{D}^{-\frac{1}{2}}\tilde{A}\tilde{D}$表示正则化的邻接矩阵；
$W^l$是一个可学习的权重矩阵；
图卷积计算为：
$H^{l+1}=\sigma\left(\tilde{D}^{-\frac{1}{2}}\tilde{A}\tilde{D}^{-\frac{1}{2}}H^{l}W^{l}\right)$
${\tilde{A}}=A+I_{N}$

添加每层的残差连接，以防止过度平滑：
$g^{l+1}=H^{l+1}+H^{l}$；
$g^{o u t}\equiv\mathrm{tanh}\left(W_{o}g^{K}\right)$；
$g^K$即最后一个图卷积层的输出，最后再加上一层FFN，作为整个图卷积部分的输出；

**关注问题的注意力**
利用问题为中心的双重注意力机制强调重要的点，去除不重要的点。
针对问题$q$，计算与源文档节点之间的交叉注意力，第一次注意力为：
$\alpha_{j}=\frac{\exp\left(\delta\left(q,g_{j}\right)\right).}{\sum\exp\left(\delta\left(q,g_{k}\right)\right)}$
其中$\delta$表示注意力函数，$q$表示问题的hidden表达，$g_i$是点i的最终表示；
GCN输出为$<v_0, v_1,...,v_n>$

在初始状态$t_0$时，解码器生成一些列的摘要token，$y_{1},y_{2},\cdot\cdot\cdot\ ,\ y_{m}$。
计算token与结点之间的交叉注意力：
$t_{i}=R N N\left(t_{i-1},\,c_{i-1}\right)$
$\beta_{j}=\frac{exp(\delta(t_i, g_i))}{\sum exp(\delta(t_i, g_i))}$
其中$\delta$表示注意力函数，$t_i$表示状态i的hidden表示，$g_i$表示节点i的最终表示。
最终计算的注意力应为：
$\psi_{i}={\mathrm{softmax}}\left(\gamma{\alpha_{i}}\ +\left(1\,-\,\gamma\right){\beta_{i}}\right)$
$=\,\frac{\exp\left(\gamma\alpha_{i}+(1-\gamma)\beta_{i}\right)}{\sum_{k\in[1,n]}\exp\left(\gamma\alpha_{k}+(1-\gamma)\beta_{k}\right)}$
$\psi_{i}$表示最终注意力，$\gamma\ \in\ [0,1]$表示软连接，计算方式可由一个简单的神经网络训练：
$\gamma\ =\,\sigma\,\left(w^{T}\bigl[\alpha;\beta\bigr]+b\right)$

最终全文的语义向量表示为：
$c_{i}=\sum\psi_{j}v_{j}$

因为概念v可能出现在摘要中，仍需要添加复制机制；
$y_{i}=\mathrm{softmax}\left(W_{o}\left(\mathrm{tanh}\left(W\left([t_{i};c_{i}]\right)+b\right)\right)\right)$表示根据词表生成对应的结果；
$p_{c o p y}= \sigma\left(W_{c o p y}\left[t_{i};\,c_{i}\right]\right)$表示copy的概率；
$p=\left(1-p_{\mathrm{copy}}\right)\times y+p_{\mathrm{copy}}\,\times\psi$表示最终生成的概率；值得注意的是，这里不能处理OOV词汇


# 006
英文：
中文：
2022
## 问题和现状
## 创新和方法
## 实验和结论
## 自己想法