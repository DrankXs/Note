# 001*

英文：Length Control in Abstractive Summarization by Pretraining Information

Selection

中文：通过预训练的信息选择来实现的抽象摘要长度控制

##  痛点及现状 

以往的长度可控的摘要模型：解码阶段控制长度。

1. 编码阶段对从源文档中选择信息设计长度不敏感。
2. 倾向于生成与训练数据长度相同的摘要。
  两阶段法（1.抽取l个句子；2.用双向encoder进行编码）：

3. 中间阶段引入噪声
4. 缺失1阶段的长度信息

##  方法及创新 

* 提出长度感知注意机制（length-aware attention mechanism, LAAM），实现适应期望长度的原编码。
  原理：summary的长度其实与Document是相关的，即summary长度相似，则其对应Document必定存在相似的地方。

方法：从原始数据构建summary长度平衡的数据集LBD，然后训练LAAM。

##  实验及结论 

优于目前最先进的长度可控方法，在CNN/Daily Mail和XSUM，评估手段：rouge。

同时在LBD上训练的LAAM优于普通训练的LAAM。

## 方法总结

提出了一个新鲜的思路，其实质是针对transformer中的cross-attention部分做了处理，基于期望长度$$l$$进行了一个加权，使其更关注自己期望的信息。

自己针对该方式尝试做个延展，但发现泛用性其实不高，其缺点如下：

* 加权方式是由人设计的，基准是额外的输入期望长度$$l$$，不是从Document总结出来的。
* 其中能够处理的仅是表层的位置，其深处的含义需要人们自己处理。

# 002*

英文：Improving Factual Consistency of Abstractive Summarization via Question Answering

中文：通过“问答”来提高抽象摘要的事实一致性

##  痛点及现状 

生成的摘要实际上可能与输入文档的含义不一致；

自动评价事实一致性的指标模型协议QAGS冗杂，消耗大量内存，浪费大量时间；

##  方法及创新 

1.提出一个有效的自动评价指标来衡量事实一致性，是对QAGS协议的简化QUALS；

2.提出一个新的学习算法，可以在模型训练过程中最大化提出的度量。

##  实验及结论 

提高了摘要的整体质量和事实一致性，评判指标是自动指标和人工评价。

XSUM 和 CNN/DailyMail为数据集；

XSUM上，QUALS-CONSEQ比BART-large MLE基线高出4个点；

CNN/DailyMail高出2个点。

##  方法总结 

* 简化了事实一致性的神经网络评估模式---生成Q-A对。即QUALS评价模式
* 采用了比对学习，很好的描述了Seq2Seq比对学习的流程：
    1. 首先训练一个能够完成大目标（这里指摘要生成）的模型，其训练目标损失函数为loss_func
    2. 其次构建一个表示我们期望趋势（这里指事实一致性）的正负样例数据集
    3. 最后实现迭代训练，使得模型能够在完成大目标的基础下，趋势愈发明显。
* 强化学习的使用：深度强化学习并不复杂，需要两个工具：1，评价指标；2，基线。

# 003*

英文：BASS: Boosting Abstractive Summarization with Unified Semantic Graph

中文：使用统一语义图提升抽象摘要

## 痛点及现状 

对于Seq2Seq体系，长文档多文档摘要仍然困难。

## 方法及创新

1.针对Document构建图，再结合图和原文本共同生成摘要，针对图的生成，尤其关注共指短语的构建。

2.提出基于图的Encoder-Decoder，利用图结构改进文档表示和摘要生成过程。

## 实验及结论

在数据集BIGPATENT和WikiSUM上，取得了比传统模型更优的效果。

利用rouge-F1和BERTScore

# 004

英文：Focus Attention: Promoting Faithfulness and Diversity in Summarization

中文：集中注意力：提升摘要的准确性和多样性

## 痛点及现状

摘要是基于整篇文档编写的，这与大多数seq2seq解码器形成了对比，后者在每个解码步骤中都要学习关注突出的内容，同时决定生成什么。

这里按照我的理解，应是seq2seq时不时生成 常见且与输入无关的回答，该情况常出现于问答问题里，摘要不是那么明显，但仍存在。

## 方法及创新

1.引入Focus Attention机制，期望decoder生成与输入文档相似或相关的token；

2.提出Focus Sampling方法，用于支持生成不同的摘要；

## 实验及结论

在BBC摘要任务中，使用两个Focus Attention增强的模型比原始模型生成的摘要更接近目标，更忠于输入文档，在rouge和多种得分上更优。

提升不是很明显。

## 方法总结

1.focus attention本质上是利用cross-attention连接encoder，deocder部分的特性，加强了encoder输出的影响到decoder生成部分，个人感觉倾向于残差网络的方式，防止信息丢失。

2.其将token级的focus输出（词表分布）进行平均，得到文档级的词表分布。

3.通过文档级的词表分布，直接得到了文档中的关键词，同时添加常用功能词，避免使用其它词来生成摘要。

# 005

英文：“Why Should I Trust You?“Explaining the Predictions of Any Classifier

中文：缘何信你？针对任意分类器预测结果的解释

## 痛点与现状

机器学习模型被认为是黑匣子，难以理解预测背后的原因。

如果用户不信任模型或预测，将不会使用它。

信任一个预测：用户是否充分信任一个个人预测，从而采取一些基于该预测的行动

信任一个模型：用户是否信任一个模型在部署后能够以合理的方式行事。

这两个问题都直接受到人类对模型理解程度的影响。

## 方法与创新

1.提出新的解释技术LIME：通过一个可解释模型的局部逼近，以可解释和忠实的方式解释任何分类器的预测。

2.SP-LIME，一种通过子模块优化 选择一组具有解释的代表性实例来解决“信任模型”问题的方法。

3.通过模拟实验和人类实验进行综合评估，衡量解释对信任和相关任务的影响。

通过实验，使用LIME的普通人能够从多个分类器中选择出哪个分类器在现实世界中的泛化效果更好，还能改进在20个新闻组上训练的不可信分类器。

## 解释模型的必要性

解释预测：即呈现文本或视觉上的工件，以提供对实例组件（例如文本中的单词，图像中的补丁）与模型预测之间的关系的定性理解。

如果解释是忠实和可理解的，那么解释预测是让人类信任和有效使用机器学习的一个重要方面。

人们通常有关于领域内的先验知识，如果他们理解预测背后的原因，就可以使用这些知识来接受预测或拒绝预测。

对于分类模型的解释：

为对模型整体信任度进行一定的度量，往往使用查看示例的方式来评估模型的真实性，建议解释一个模型的几个代表性的个体预测，作为一种提供全局理解的方式。

模型或评估出错的方式：

1.数据泄露；

2.数据集移位。

希望能够评估两个或多个模型之间的相对信任度，不是通过验证集的准确率来判定，而是通过解释示例判断。

我们希望解释模型的特征必须可理解。

# 006

英文：Dissecting Generation Modes for Abstractive Summarization Models via

Ablation and Attribution

中文：通过消融和属性剖析生成模型中的抽象摘要模型

## 痛点及现状

不清楚神经抽象摘要模型的原理，其如何形成摘要，如何确定摘要均不了解

## 方法及创新

提出了一种两步解释摘要模型决策的方法：

1.消融完整模型分析模型的行为，将对应的decoder决策分类为几种生成模式（判断其与输入的关系，是强依赖与输入，还是相关甚少，或是介于两者之间）；

2.分离出来依赖于输入的决策之后，使用几种不同的归因方法来解释这些决策。

通过技术中，选择内容的能力和从输入扰动中重建预测的能力来比较技术的优劣，从而揭示重要属性是否对于下个令牌的生成真的重要。

1.首先粗略的将生成决策分成几种生成模式，在确认模型有鲁棒性之后，通过多模型消融的预测下一个token的方式来探测模型

选定的多种模型模型：

1.无输入的基础语言模型 $$LM_{ \emptyset}$$

2.无输入的摘要模型 $$S_{\emptyset}$$

3.部分文档输入的摘要模型 $$S_{part}$$

4.全文档输入的摘要模型 $$S_{full}$$

这些消融可以告诉我们什么时候决策是上下文无关的，什么时候是严重依赖于上下文的。

2.在模型严重依赖于源文档的时候，我们希望关注细粒度的决策归因。

研究了若干技术的解释：遮挡，注意力，综合梯度和输入梯度。

提出了基于反逻辑和部分输入，在输入子集下来定量评价模型的表现。
