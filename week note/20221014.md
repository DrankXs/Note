# 001
英文：Timeline summarization based on event graph compression via time-aware optimal transport.  
中文：使用时间感知最优传输理论的基于事件图表示的时间轴摘要  
2021

## 痛点与现状

时间轴摘要从新闻集合中识别主要事件，并按时间顺序描述，并且tag有关键日期；  
传统方法往往根据每个时间tag单独生成摘要，忽略结构间的事件发展；

## 创新与方法  
将新闻文章表示为事件图，使摘要任务变成将整个图压缩到其显著子图里；

使用共享参数和时间顺序连接的事件描述了时间轴的骨架，包含了全局事件图中语义相关，结构突出和时间一致的事件；引入时间感知的最佳传输距离，用无监督的方式压缩模型。

多文档编码的事件图构造；  
使用最佳传输的无监督事件图压缩；  
时间感知的Gromv-Wasserstein distance

## 实验与结论  
在3个真实数据集上显著提高了技术水平，包括两个公共标准基准测试和新收集的TimeLine100数据集。

## 自己想法  
考虑了图；
提出了最优传输这个图相似度比对方法；  
提出了时间感知和文本结合的方式；

# 002*
英文：Structured Neural Summarization
中文：结构化神经摘要
2021

## 痛点与现状  
将长序列总结为简明语句是自然语言处理的核心问题；
将摘要的重点放在Decoder上，注意力机制和copy机制；

## 创新与方法  

开发一个框架：用一个图组件来扩展已经存在的句子序列encoder，从而推理弱结构化数据中的长距离关系；
**自动摘要关注实体和其之间的关系，忽略冗余和公共概念；**

## 实验与结论  
在一个广泛的评估中，混合序列图模型比单纯的序列模型和纯图模型效果好。
MethodNaming：
&emsp;数据集：Java(small)+C#（自己从github上的开源代码中收集）;
&emsp;评估：针对subtoken的F1分数；rouge-2和rouge-L也被使用；  
MethodDoc:
&emsp;数据集：python（Barone&Sennrich）
&emsp;评估：F1，ROUGE-2和ROUGE-L；
NL摘要：
&emsp;数据集：CNN/DM；
&emsp;评估：Rouge；

## 自己想法
之前考虑用图来压缩长语句有些不现实，直接从语句构造图的方法有多种，其中越简便，构造的图越大，反而增加了处理难度，虽然图神经网络的单节点运算特点可以使得关注长语句的所有内容，避免了普通序列encoder的缺陷，但是带来了计算效能的大幅度降低；  
令人欣喜的是，本文中提到了这一点，使用了另一篇文章Learning to represent programs with graphs.所述的图压缩算法，因此可以联想，使用固定的图压缩算法，则可以将原始图$G_{src}$和摘要图$G_{sum}$使用同样规则压缩，成为原始图$G_{src}'$和摘要图$G_{sum}'$, 在比较两图之间的差异时，就可以减少对应的偏差，即$different(G_{src}, G_{sum}) \sim different(G_{src}', G_{sum}')$；  
自注意力可以替代RNN序列编码体系结构；

# 003（荐）
英文：Towards Zero-Label Language Learning
中文：面向0样本语言学习
2021

## 痛点与现状  
NLP任务常常面对0标签的情况；

## 创新与方法  
提出新的方法，更好利用强大的PLM；
无监督数据生成（UDG），利用few-shot合成训练数据，而不是人工注释；

## 实验与结论  
与标记数据混合使用，在SuperGLUE中得到最新的结果。

# 004
英文：The Use of MMR, Diversity-Based Reranking for Reordering Documents and Producing Summaries 
中文：MMR的使用，基于多样性的重排序方法，针对重排文档和摘要生成
1998

## 痛点与现状  
自动摘要的必要性

## 创新与方法  
将query的相关性和信息新颖性结合，提出最大边际相关性MMR；  
MMR追求保持相关性的同时减少冗余；  

## 实验与结论  
初步结果表明，MMR多样性排序有一定的优势；
尤其是构建非冗余的多文档摘要时。  

# 005
英文：Knowledge graph-augmented abstractive summarization with semantic-driven cloze reward.
中文：基于语义驱动的完型填空奖励式的知识图谱增强抽象摘要
2020

## 痛点与现状  
Seq2Seq生成的摘要往往存在事实一致性的问题，并且是近乎于提取的方式；  
表明现有的波形缺乏对输入的语义解释；  


## 创新与方法  
摘要器应该通过输入获得语义解释，比如使用结构化的表示，生成更有信息的摘要；  
提出抽象摘要新框架，ASGARD，基于图形化和语义驱动奖励；  
使用双重编码器--一个顺序文档编码器，一个图形结构编码器；
设置多个完型填空式的奖励；  
使用OpenIE;

## 实验与结论  
在NYT和CNN/DM中，模型比没有知识图输入的变体产生了更高的Rouge得分；

## 自己理解
图对NLP的作用相当于利用人类外部知识增强token理解的过程；

# 006 
英文：InfoLM: A New Metric to Evaluate Summarization & Data2Text Generation
中文：InfoLM:评估摘要和数据文本生成的新指标
2022

## 痛点与现状  
人工注释摘要十分昂贵；
自动评价指标往往依赖精确匹配，难以处理同义词；

## 创新与方法  
引入infoLM一类未经训练的度量，其是基于字符串的度量，使用预训练的MLM解决问题；
infoLM包括：1，预训练遮蔽语言模型PMLM，计算词表上两个离散概率分布，在给定候选句和参考句的情况下，观察到词汇表的每个标记的概率；2.对比函数$\mathcal{I}$，用于测量上述概率分布之间的不相似度。

## 实验与结论  
通过直接评估，证明了InfoLM在统计上取得了显著的改进，并在许多配置中在摘要和数据文本生成方面获得了超过10点的相关增益；
