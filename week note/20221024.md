# 001
英文：Graph Enhanced Contrastive Learning for Radiology Findings Summarization
中文：针对放射学研究摘要的图增强比对学习
2022
## 痛点与现状
放射学报告的impression部分总结了来自调查结果部分最突出的发现，也是同其它放射学研究者之间交流的最重要部分；
这种专业性极强的总结需要专家，且浪费时间；
encoder-decoder架构探索将额外的知识纳入其中，但是单独的将输入作为额外知识encode局限较大；

## 方法与创新
提出了一个统一框架，综合利用额外知识和原始发现，从而使用适当方式提取关键信息，促进印象生成；
具体来说：对于每个输入结果，使用encoder进行encode，同时使用实体和其依赖树构造一个图；
之后利用图编码器对构建图中的关系信息进行建模；
最后，使用比对学习增强对关键词的学习（分别屏蔽关键词和非关键词）。

## 实验与结论
在OpenI 和MIMIC-CXR上证实了方法的有效性。

## 自己想法
使用了一个对比学习的训练目标（从其他paper来的），对比学习的训练目标是多种多样的，这个目标的提出可以给出一个如何科学使用对比学习的样例；
CheXBert可以用于检测文本的事实一致性；
stanza提取医疗实体和依赖树；

# 002
英文：PRIMERA: Pyramid-based Masked Sentence Pre-training for Multi-document Summarization
中文：PRIMERA：针对多文档摘要的金字塔状遮盖语句预训练
2022

## 痛点与现状
最新最优的多文档摘要模型是基于图的:
抽象的语义表征，语篇图，特定于数据集的定制架构难以使用PLM；
现有的预训练模型要么使用单文档预训练目标，要么使用仅针对编码器的造型，这些模型不适用于摘要等生成任务；


## 方法与创新
PRIMERA，针对多文档表示的预训练模型；
提出多文档摘要的一个预训练方式，减少了对特定领域数据的体系结构和微调token需求；
训练目标是联系和聚合跨文档信息；
使用有效的encoder-decoder transformers，简化输入文档的处理；
Gap Sentence GenerationGSG句子遮盖策略：Entity pyramid，遮盖重要句子来预测。

## 实验与结论
在3个不同领域的6个多文档数据集下，实验了zero-shot，few-shot和full-supervised，具有较大的优势；
LED：最大长度4096/1024,滑动窗口w = 512；
数据集：Multi-News, Multi-XSci, WCEP, WikiSum, arXiv, DUC2004;
适用ROUGE评估（AVG ROUGE：R-1,R-2,R-L）
比较：BART；PEGASUS，LED模型。

# 003
英文：Longformer: The Long-Document Transformer
中文：Longformer:长文档Transformer
2020
## 痛点与现状
Transformer由于self-attention无法处理长序列，self-attention的复杂度是$O(n^2)$;  

## 方法与创新
Longformer，attention复杂度随输入长度线性增长；
Longformer的attention结合了local windowed attention和任务驱动的全局注意力；  
介绍了LED，Longformer-Encoder-Decoder，支持长文档的Seq2Seq；

## 实验与结论
使用Longformer测试了字符级的语言模型，在text8和enwik8上获得了最优结果；  
对其预训练并测试了多个下游任务；  
长文档任务中，始终优于RoBERTa，在WikiHop和TriviaQA上设置了新的最先进的结果。

## 自己想法
attention机制是Transformer的核心，从原本的线性层方式加注attention，到后来self-attention和cross-attention都增强了transformer的应用。
Longformer通过选中指定位置进行global attention来减少attention的复杂度。
是否可以根据依赖树的依赖关系来选择attention的操作？这样做的效果会怎样？会不会导致attention的断层。

# 004（荐）
英文：ZEROGEN: Efficient Zero-shot Learning via Dataset Generation
中文：ZEROGEN：通过数据生成的高效zero-shot学习
2022
## 痛点与现状
大规模预训练模型PLM具有优越的生成能力，数据集生成逐渐成为一个课题；

## 方法与创新
灵活高效的zero-short学习方法：ZEROGEN；
给定zero-shot任务，用PLM以无监督方式生成一个dataset；之后，在合成数据集的监督下训练小任务模型；
最终的模型与PLM相比，只是少了大量的参数却拥有同样的高效推断能力；
ZEROGEN可以为无数据模型的知识蒸馏和无参考文本的生成评估提供有用的见解；
利用Text2Text来连接PLM和最终下游任务；

## 实验与结论
不同的NLP任务中，实验了有效性；
有6个数据集，3个不同的NLP任务；

## 自己想法
里面有一句话很不错，特定prompt反映了特定任务的基本知识，不如说prompt这一机制本就是为了模型在训练时增加的新的知识，无论是人工模板设置的prompt还是机器自动生成的prompt。
TAM（小的任务模型）的有效性表示了混合复杂的知识模型其实并不如专精某项的小模型，虽然混合复杂知识能够解决多个问题。
prompt的限制体现在强制在PLM中，是因为PLM存在将文本知识学习的过程，从而实现prompt的作用，但是针对未经训练的小模型，prompt收获的作用微乎其微，除非能够根据不同样例设置不同的prompt，实现一个区分，学习到prompt的知识。


# 005
英文：
中文：
2022
## 痛点与现状

## 方法与创新

## 实验与结论

## 自己想法


<br><br><br><br><br><br><br><br>
1