# 001
英文：Chain-of-Thought Prompting Elicits Reasoning in Large Language Models
中文：大规模语言模型下的思维链提示引发推理
2023
## 问题和现状
大规模预训练模型不断发展。
仅扩大模型存在局限性，难以处理挑战性任务，如算术、常识和符号推理。
1.算术推理技术可从自然语言原理处理；但是缺少高质量数据集。
2.LLM的性能允许使用prompt来进行few-shot训练来达成目标。使用“输入-输出”的提示可以方便的提升性能。但推理任务上表现不佳。

## 创新和方法
思维链提示，来提升大规模语言模型执行复杂推理的能力。
提供了一系列示例。
认为LLM有简单的方法来解锁LLM的推理能力。
给定三元组<input, chain of thought, output>

## 实验和结论
思维链提示在3个LLM表名，提高了推理任务中的表现。
在GSM8K，使用PaLM 540B的思维链提示大大优于标准提示。

## 自己思考
可以看出，如果为LM增加足够多的人类思考信息，是能够将LM的性能进一步提升的。
这里的思维链相当于补充信息的过程。
思维链的构建自然也可以更精简，但推测简单的思维链逻辑更严谨，则导致训练出来的推理结论更加专一，不能广泛的学习到推理能力。


# 002
英文：Large Language Models are Zero-Shot Reasoners
中文：LLM是无样本推理机
2023
## 问题和现状
LLM在few-shot表现优异；
CoT(Chain of Thought)在推理方面表现优异；

## 创新和方法
简单的在答案之前添加"Let's think step by step"，证明LLM是出色的zero-shot推理机。

## 实验和结论
zero-shot的CoT,使用相同的prompt模板，在多个推理任务中取得优于普通zero-shot LLM的结果。

## 自己想法
这个prompt通过增加中间步骤，促使模型进行推理，从而得到最终的结果；
从文中结论可以看出，LLM已经有了充足的推理能力，但缺乏诱因，如果告诉LLM进行推理，它可以进行推理，但由于目标为单纯的思考/推理，不会得到结果。
如果希望得到结果，利用prompt，类似于人类的直接不思考得到答案。
那么LLM和人脑能否更加接近呢？人脑针对各种问题会自行的进行分辨是否进行复杂推理，相当于搜索的过程，但LLM倾向于one by one，一步步，缺乏对具体问题进行分辨的能力。

# 003
英文：PaLM: Scaling Language Modeling with Pathways
中文：PaLM：使用路径进行语言模型的缩放
2022
## 问题和现状
LLM具有出色的few-shot能力

## 创新和方法
训练了一个多参数，密集激活的Transformer语言模型，称为Pathways Language Model；
大规模使用Pathways,这是一种新型的ML系统，可以高效的在多个芯片上训练单个模型。
该模型更倾向于分布式训练ML模型

## 实验和结论
PaLM 540B在一组多步推理任务中表现优于最先进的优化状态。

## 自己想法
本质上PaLM是期望做到一个庞大规模的模型，但与此同时，实现规模日益增加的大模型的加速训练；
这里加速的模型即Transformer架构的模型。
使用了一些加速Transformer架构的技巧，虽然损失一部分精确度，但是在庞大的训练规模下，这些被弥补。
SWiGLU,是文中提到的激活函数，可以提高质量，回来可以简单的应用在自己的网络中。

# 004
英文：P-Tuning: Prompt Tuning Can Be Comparable to Fine-tuning Across Scales and Tasks
中文：P-Tuning：提示调优可以于在跨规模和任务的情况下与微调相媲美
2022
## 问题和现状
prompt调优只对frozen LM进行调优；微调训练消耗内存；
prompt调优对于正常大小的PLM表现不好；
prompt调优不能处理硬序列标记任务。

## 创新和方法
优化prompt调优，在大部分规模的模型和NLU任务中有效。
与普通的微调相比，只用调优0.1%-3%的参数。
对PLM的每一层添加Prompt调优（即连续prompt embedding）

## 实验和结论
P-Tuning增加了连续prompt调优的能力，缩小了各种设置之间进行fine-tuning的差距。
P-Tuning在多个规模的模型下对多个NLU任务实现了匹配上fine-tune的性能。

## 自己想法
这是简单的将原本的连续prompt-tuning应用到每个层上进行prompt，提高prompt参数量的同时实现了更泛化的应用。
prompt的长度/位置都会对结果造成影响。

# 005
英文：AUTOMATIC CHAIN OF THOUGHT PROMPTING IN LARGE LANGUAGE MODELS
中文：LLM中的自动思维链提示
2022
## 问题和现状
LLM可以生成中间推理步骤（CoT）来进行复杂推理。
CoT可以通过简单的prompt"Let us think step by step"实现生成CoT，或者手工输入对应的CoT prompt;
Zero-Shot的CoT经常会出现错误。
## 创新和方法
分析得到，演示问题的多样性是Zero-Shot-CoT推理链是否错误的关键
提出一个自动的CoT方法：AutoCoT，它自动构造演示，对多样性问题抽样，生成推理链来构建例子。
Auto-CoT的两个步骤：1.将给定dataset的问题划分为几个集群；2.从每个聚类中选择一个具有代表性的问题，并使用简单的启发式Zero-Shot-CoT生成其推理链。
## 实验和结论
在GPT-3的10个公共推理任务中，Auto-CoT的性能大于等于手动CoT的性能。
## 自己想法
探寻了few-shot样例对Zero-Shot-CoT的影响，发现Zero-Shot-CoT推理链的形成是需要分散讨论的，类似的样例反而会影响模型的推理能力。


## 背景知识
**CoT**：思维链，辅助逻辑推理问题，即推理时描述推理过程的语言。
&emsp;zero-shot-CoT:利用LLM生成对应的CoT;
&emsp;Manual-CoT:手工设置CoT.
**In-Context Learning**
ICL为few-shot Learning，将几个demo样例（x1,y1,x2,y2,x_now）作为输入的一部分，可以提高LLM在下游任务上的性能。演示（demo样例）的选择会对结果造成很大的差异。
&emsp;选择与输入相关紧密的样例作为demo样例；
&emsp;增加细粒度的信息；
&emsp;控制LLM的输出概率，而不是直接计算目标标签的可能性。

## Auto-CoT
关键在于自动构造好的问题和其推理链。
在选择问题的时候，可以运用相似性检测抽样演示问题。

Retrieval-Q-CoT，基于余弦相似度选取top-K个相似问题。
Random-Q-CoT:随机选取K个测试问题。
它们均使用zero-shot-CoT来生成推理链。

Retrieval-Q-CoT效果并不是那么好，由于类似的样例，可能会诱导采用类似的推理模式推理所有问题。

将问题划分至K个集群的时候，存在一个频繁出现Zero-Shot-CoT错误的集群。

选择样例足够多样性，能够提升CoT的性能。

Auto-CoT包含两个阶段：
&emsp;1.问题聚类：将给定数据集的问题划分为几个聚类；
&emsp;2.示范抽样：从每个聚类中选择一个具有代表性的问题，再用简单的启发式方法使用CoT生成对应的推理链。

问题聚类：
&emsp;1.使用Sentence-BERT计算问题的句向量，使用平均向量表示
&emsp;2.K-Means聚类，聚成K个簇；
&emsp;3.对每个聚类i中的问题，按照距聚类中心的距离，得到一个列表。

示范抽样：
为抽样问题集簇生成推理链，满足选择标准的样本演示（即包含问题，关系推理描述，结果的内容）；
&emsp;1.针对问题集簇i，迭代搜寻符合要求的问题j，要求问题尽可能靠近聚类中心。
&emsp;2.选取好问题后，用Zero-Shot-CoT的方式，生成对应的思维链，进而继续生成对应的结果。
&emsp;3.连接问题，思维链，和结果，得到样例。（可以在这一步进行处理，从而判断是否使用这个示例）




# 006
英文：RefSum: Refactoring Neural Summarization
中文：RefSum：重构神经摘要
2021
## 问题和现状
不同系统之间存在潜在互补性，但很少有研究视图研究文本摘要中的这一情况。
两阶段方法存在缺点：场景过于特定；两阶段之间学习模型存在间隔；训练测试数据存在分布差异

## 创新和方法
强调了以前方法的几个局限性，这促使我没提出了一个新的框架Refactor，提供了文本摘要和摘要组合的统一视图。
1.分析了不同系统之间互补性时影响两阶段学习性能的主要因素（Base-Meta Gap|| Train-Test Gap）；
2.展示了促进two-stage之间的交流可以缓解two-stage的问题；
3.进行了全面的实验来证明了思路。

## 实验和结论
进行了全面评估，涉及22个基本系统，4个数据集，3个应用场景。

## 自己想法
与摘要关系不大，实质做的操作是将两阶段的模型融合到一个模型里，从而避免两阶段处理时的固有缺陷

## Summarization in Two-Stage
**Base-System**
文档$D=\{s_{1},\cdot\cdot\cdot,s_{n}\}$
有$n$个句子；
$C$为生成的候选摘要；
公式为：$C={\mathrm{BASE}}(D,T,S,\Theta^{\mathrm{base}})$
${\mathrm{BASE}}({\mathrm{,}}\mathbb{\Theta}^{\mathrm{base}})$为基础系统；
${\mathcal{T}}$为训练方式，$\mathcal{S}$为解码策略。

**Meta system**
不同的$\mathbf{BASE}(\cdot)$，${\mathcal{T}}$，$\mathcal{S}$，将导致不同的$C\;=\;\{C_{1},\cdot\cdot\cdot\;,C_{k}\}$。
Meta system主要结合$C$之间的互补性，得到更优结果；
即，给定$C\;=\;\{C_{1},\cdot\cdot\cdot\;,C_{k}\}$，重构一个新的候选摘要$C^{*}$
$C^{*}=\mathrm{META}(D,C,\Theta^{\mathrm{meta}})$

## 重构文本摘要
为解决two-stage的问题，(i) Base-Meta Learning Gap (ii) Train-Test Distribution Gap.

Refactor模型统一Base-system和Meta-System的目标，选择文档句子的最佳组合从而生成摘要。

使用一组参数来将base-system和meta-system的学习过程参数化，则Refactor模型为$C^{*}=\mathrm{REFACTOR}(D,C,\Theta^{\mathrm{refactor}})$；
其中$\mathrm{REFACTOR}\left(\cdot,\Theta^{refactor}\right)$为Refactor模型，$\mathcal{C}$为候选摘要集合。

Refator先从文档句子中选择候选摘要(pretrained-Refator)，再学习从不同的系统输出中选择候选摘要(fine-tuned Refactor)。

**Pre-trained Refactor**
*输入*
将文档和候选摘要作为输入
$D\;=\;\{s_{1},\cdot\cdot\cdot\;,s_{n}\}$
$C\,=\,\{C_{1},\cdot\cdot\cdot\,,\,C_{m}\}$
$\mathrm{REFACTOR}\left(\cdot,\Theta^{refactor}\right)$将实例化为一个评分函数，量化了候选摘要$C_i$与源匹配的程度。
$C^{*}=\mathrm{REFACTOR}(D,C,\Theta^{\mathrm{refactor}}) \\ = argmax_{C_i \in C}(SCORE(D, C_i))$
$D$表示文档，$C_i$表示摘要。

*SCORE*
求$SCORE(\cdot)$,基于贪婪匹配算法定义，
文档嵌入矩阵为：
$D = \left\langle{\bf d_{1}},\cdot\cdot\cdot\cdot\ ,\ {\bf d_{k}}\right\rangle$
候选摘要嵌入矩阵为：
$C = <c_1, ..., c_l>$
两矩阵均由BERT编码。
其中
$\mathrm{SCORE}(\mathrm{D},\mathrm{C})=2\frac{\mathrm{R}(\mathrm{D},\mathrm{C})\cdot\mathrm{P}(\mathrm{D},\mathrm{C})}{\mathrm{R}(\mathrm{D},\mathrm{C})+\mathrm{P}(\mathrm{D},\mathrm{C})}$
$\mathrm{R}({\bf D},{\bf C})=\frac{\sum_{i}w_{i}\,{\bf m}\!{\bf a}{\bf x}_{j}\,{\cos}({\bf d}_{i},{\bf c}_{j})}{\sum_{i}w_{i}}+1$
$\mathrm{P}(\mathrm{D},\mathrm{C})=\frac{\sum_{j}\mathrm{max}_{i}\cos(\mathrm{d}_{i},\mathrm{C}_{j})}{l}\,+\,1\,$

而$w_i$为第i个token的权重。引入一个两层的Transformer构建权重模块：
$w_i = \frac{exp(dot(\boldsymbol{d}_i, \hat{\boldsymbol{d}}_0) / \sqrt{d})}{\sum_j exp(dot(\boldsymbol{d}_j, \hat{\boldsymbol{d}}_0) / \sqrt{d})}$
$\hat{\boldsymbol{D}} = Transformer(\boldsymbol{D})$,其中$\hat{\mathbf{d}}_{0}\ =\ \mathbf{D}[0]$表示[CLS]token。

*训练目标*
使用排名损失来学习参数$\Theta_{refactor}$,
$\begin{array}{c}{{L=\sum_{i}\sum_{j\gt i}\operatorname*{max}(0,S\mathrm{CORE}(\mathrm{D},\mathrm{C}_{j})}}\\ {{}}&{{-\mathrm{SCORE}(\mathrm{D},\mathrm{C}_{i})+(j-i)\ast\lambda_{c})}}\end{array}$
$C_i$和$C_j$表示第i和第j个候选样例，由ROUGE分数递减排序。
$\lambda_c$是超参数

**Fine-Tuned Refactor**
微调时选用的候选摘要是由base-system在不同应用场景下生成的。




# 007
英文：Leveraging Graph to Improve Abstractive Multi-Document Summarization
中文：利用图来提升多文档抽象摘要
2020
## 问题和现状
捕捉文本信息的图有利于从多文档中检测突出信息和生成整体一致的摘要。

## 创新和方法
开发了一种抽象多文档摘要模型，利用similarity graph和discourse graph，更有效的进行多文档摘要的生成。
开发了一个图信息注意力机制，将其合并到Encoder中，使得Encoder能捕获丰富的跨文档信息，Encoder限制很小，可以是大多数的PLM。

## 实验和结论
在WikiSum和MultiNews数据集上的结果表明，所提出的架构在几个强大的基线上带来了提升。

## 自己想法


## Model
首先将源文档拆分成多个段落，然后以段落为单位构造文档的图形表示。（比如：根据段落tf-idf表示之间的余弦相似度来构建相似度图）
用$\mathbb{G}$表示输入文档的图表示矩阵；
$\mathbb{G}[i][j]$表示$P_i$和$P_j$之间的关系权重。
任务为利用$L$个文档的集合$P_1, ..., P_L$生成摘要$S$。

模型是Encoder-Decoder结构；

**Graph Encoding Layer**
Graph Encoding Layer对所有文档进行全局编码；

*Graph-informed Self-attention*
${\mathcal{x}}_{i}^{l-1}$表示$l-1$层图编码层对文章$P_i$的编码，
$x_i^0$即输入文章的向量表达。
则对于每个段落$P_i$，context 表示为$u_i$，计算为线性变换的段落向量加权和：
$\alpha_{i j}\ =softmax(e_{i j}+\Re_{i j})$
$e_{i j}=\frac{(x_{i}^{l-1}W_{Q})(x_{j}^{l-1}W_{K})^{T}}{\sqrt{d_{h e a d}}}$
$u_{i}=\sum_{j=1}^{L}\alpha_{i j}(x_{j}^{l-1}W_{V})$
$\Re_{i j}=-\frac{(1-\mathrm{G}[i][j])^{2}}{2\sigma^{2}}$

其中$W_K, W_Q, W_V\in \mathbb{R}^{d\times d}$,是参数矩阵；
$e_{ij}$表示$P_i$和$P_j$之间的潜在关系权重，
创新点在于$\Re_{ij}$,为额外的成对关系偏差，计算的即图表示矩阵$\mathbb{G}$的高斯偏差；其中$\sigma$表示图结构影响强度的标准差，通过调优数据集来基于经验的方式设置。
$\Re_{ij} \in (-inf, 0]$度量$P_i$和$P_j$之间的紧密型。

经过上述的注意力操作，采用ReLU激活函数的两层前馈网络和高速层归一化方法，得到段落向量$x_i^l$；
$p_{i}^{l}=W_{o2}R e LU(W_{o1}(u_{i}(u_{i}+x_{i}^{l-1}))$
$x_{i}^{l}=L a y e r N o r m(p_{i}^{l}+x_{i}^{l-1})$
其中$W_{o1}\in \mathbb{R}^{d_{ff}*d}$,$W_{o2}\in \mathbb{R}^{d\star d_{ff}}$,$d_{f f}$是前向反馈层状态。

**Graph Decoding Layer**
图边使用分层图注意力来指导摘要生成的过程，它由全局图注意力和局部归一化注意力构成。
在图的decoding-layer，其它部分和Transformer架构一样。

*Global Graph Attention*
Global Graph Attention 用于捕获段落级上下文信息。
$y_t^{l-1}$表示l-1层的graph decoding layer对于第t个token 的输出；
假设每个token与相关段落对齐，其中一个位于中心位置。
使用前馈神经网络将$y^{l-1}_t$转换为对应的位置的hidden state，之后可以将其利用线性投影为标量$s_t$
$s_{t}=L\ast s i g n n o i d(U_{p}^{T}t a n h(W_{p}y_{t}^{l-1})$
$W_{p}\in\mathbb{R}^{d\star d}$和$U_{p}\in \mathbb{R}^{d}$表示权重矩阵，
$s_t$表示第t个摘要的token映射的段落中心位置。

则新的段落注意力分布为：
$\beta_{t j}=softmax(e_{t j}-\frac{(1-(\mathbb{G}[s_t][j])^{2}}{2\sigma^{2}})$
其中$e_{tj}$表示在token$y_t^{l-1}$和段落$x_j$之间的注意力权重，使用之前Encoder中的e向量计算方式进行计算。
全局注意力为所有篇章向量的和：$g_{t}=\sum_{j=1}^{L}\beta_{t j}x_{j}$

*Local Normalized Attention*
全局注意力捕捉token与所有篇章之间的关系，局部注意力则仅关注每个篇章中的token级上下文。
局部注意力独立的应用于每个段落，并用全局图注意力进行归一化。
$\gamma_{t,j i}$表示第j个段落中第t个摘要token在第i个token上的局部注意力分布，归一化表示为：
$\hat{\gamma}_{t,j i}=\gamma_{t,j i}\beta_{t j}$
则局部上下文向量可计算为所有段落中token向量的加权和：
$l_{t}=\sum_{j=1}^{L}\sum_{k=1}^{n}\hat{\gamma}_{t,j i}x_{j i}$
连接全局和局部上下文向量并进行线性变换，计算分层图注意力的输出：
${d}_{t}={ U}_{d}^{T}\lbrack g_{t},{\cal l}_{t}\rbrack$
$U_{d}\ \in\ \mathbb{R}^{2d*d}$是一个权重矩阵。


# 008
英文：
中文：
2023
## 问题和现状
## 创新和方法
## 实验和结论
## 自己想法