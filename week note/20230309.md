# 001

英文：GLTR: Statistical Detection and Visualization of Generated Text
中文：GLTR：生成文本的统计检测和可视化
2019

## 问题和现状

文本生成系统滥用；
创建虚假评论，虚假文章，误导性评论；
存在假设：系统只根据现实自然语言分布的一个有限子集生成文本；白箱设置下，我们可以接受系统分布，该特征可以通过计算模型生成输出的密度以及比较人类生成的文本捕获到。

## 创新和方法

开发了GLTR，支持人类检测文本是否由模型生成的工具；

## 实验和结论

人类测验中，GLTR提供的注释方案将人类对假文本的检出率从54%提高到72%；
实验分析的时候则利用普通的逻辑回归进行了一个训练，这是传统的 特征分析-特征提取-模型训练 范式；
之后实验分析Rank和Entropy的关系，以及不同数据集之间词的rank的分布。

## 自己思考

三个测试，提取文本的属性为token的概率，token的rank（排名）和token的熵作为检测的三个特征；
目前由于随着LLM的发展，之前常用于进行该任务的特征逐渐失效，可能需要发掘新的特征。
PS：之后尝试一下论文方法对于chatGPT数据集的作用。

# 002

英语：How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection
中文：ChatGpt与人类专家的距离？通过语料，评价和检测来比较
2023

## 问题和现状

ChatGPT的高性能；
ChatGPT的负面影响：假新闻、抄袭、社会安全问题；
人类反馈的强化学习RLHF；

## 创新和方法

从人类专家和ChatGPT收集了数以万计的比较回答，涉及开放领域、金融、医疗、法律和心理领域，构建 人类ChatGPT比较语料库；
基于HC3，研究了ChatGPT的响应特征，与人类专家的差异和差距，和LLM的未来发展方向；
如何有效的检测特定文本是否由ChatGPT生成；

## 实验和结论

测试了几个检测器在HC3数据集的表现；

## 自己思考

提取的数据更具有普适性但不具有特殊性，仍需要进一步调整；
检测利用了GLTR和RoBERTa模型，没有做较深的改进，所以仍需要进一步阅读机器文本检测相关内容；
图结构特征必然也能够辅助检测，根据文中所述，一个简单的想法即ChatGPT生成文本的图必然更加紧密，可以使用平均节点度数来表示图的紧密程度；
尽管数据中包含了若干缺陷，但我认为图的特征应该有效；

# 003

哈工大ChatGPT分析

## 背景

### 技术简述

Intruction 预训练技术；
Codex 代码预训练技术；
Instruction Tuning 指令学习；
Supervised Fine-Tuning 有监督精调；
RLHF 基于人类反馈的强化学习；
DRL 深度强化学习；

### 未来方向

1.ChatGPT 有时候会生成一些似是而非、毫无意义的答案
&emsp;（强化学习训练中缺少明确的正确答案；训练过程中一些谨慎的训练策略导致模型无法产生本应生成的正确回复；监督学习训练过程中错误的引导导致模型倾向于生成标注人员的知识而不是模型存储的知识）
2.ChatGPT对输入措辞敏感；
&emsp;（只要改变措辞模型就能生成答案，尽管它刚刚声称不知道该答案）
3.ChatGPT生成的回复通常过于冗长，并且存在过度使用某些短语的情况；
4.无法避免对有害请求的回复以及偏见问题；
5.缩小ChatGPT模型（量化；剪枝；蒸馏和稀疏化）
6.RLAIF，减少人类反馈信息；

### ChatGPT劣势

**LLM本身的限制**
1.可信性：无法保证结果的正确性；
2.时效性：无法及时加入新的知识；
3.成本高昂：训练成本，调用成本都很困难；
4.特定专业领域内表现不足；
5.由于生成策略，每次的结果不尽相同；

**数据原因**
数据的偏见性，可能生成有害内容（但实际上已经比普通人更不会添加有害和有倾向的内容了）
OpenAI掌握着数据，可能数据泄露；

**标注策略原因**
模型的行为一定程度上反映了标注人员的偏好。

**不足**
1.通用性很强，但对于特定任务比如序列标注等传统任务，微调小模型可能更好；
2.不需要LLM额外知识的NLP任务中，chatGPT并不好，比如机器阅读理解；
3.多语言任务可能并不准确；
4.先验知识过强，很难纠正ChatGPT；
5.对于常识、符号和逻辑推理问题，ChatGPT往往倾向于生成不确定的回复；
6.只能处理文本

## 核心算法

### 基于Transformer的PLM

**Encoder**
将输入序列映射到一组中间表示；
通常选择MLM任务预训练；
典型的有BERT, ALBERT, RoBERTa；
**Decoder**
将中间表示转换为目标序列；
GPT：无监督自回归预训练；
GPT2：为输入添加了对应的任务表示，辅助输出预测；
GPT3：prompt添加，更大；
**Encoder Decoder PLM**
BART：噪声自监督；
T5：T2T，实现多个任务都转为T2T格式；
Switch Transformer：将混合专家网络MoE的条件运算思想引入；增加模型尺寸而不增加推理时的运算量；

### 提示学习和指令调优

提示学习（Prompt Learning）简单来说是通过一些方法编辑下游任务的输入，使其形式上模拟模型预训练过程使用的数据与任务。拉近了测试分布与预训练分布的距离。
小样本场景下的语境学习（In-context learning），即在提示中加入几个完整的例子；
指令精调（Instruction Tuning）可以说是提示学习的加强版。两种学习方法的本质目标均是希望通过编辑输入来深挖模型自身所蕴含的潜在知识，进而更好的完成下游任务。指令学习不再满足于模仿预训练数据的分布，而是希望通过构造“指令（Instruction）”并微调的方式，学习人类交互模式的分布，使模型更好的理解人类意图，与人类行为对齐。在指令学习中，模型需要面对的不再是单纯的补全任务，而是各种不同任务的“指令”，即任务要求。
指令调优为高级prompt，该prompt将很好的反应用户需求，由任务/样例-prompt变为需求-prompt；

### 思维链CoT

在小样本提示学习的示例中插入一系列中间推理步骤，有效提升了大规模语言模型的推理能力

### RLHF

**1.奖励模型训练**
获取拟合人类偏好的奖励模型。
奖励模型以提示和回复作为输入，计算标量奖励值作为输出。
&emsp;1.采样多个回复；2.将回复两两组合构成奖励模型样本，人类给出倾向性标签；3.计算两个回复的奖励值之差计算倾向性概率拟合人类标签。
**2.生成策略优化**
根据习得的奖励模型，在强化学习的框架下训练ChatGPT/InstructGPT的参数。

# 004

英语：Deep Reinforcement Learning from Human Preferences
中文：从人类偏好中深度强化学习
2017

## 问题和现状

让复杂的强化学习与现实环境有效交互，需要向系统传达复杂目标；

## 创新和方法

探索了trajectory segments之间的非专家人类偏好定义的目标；
要求人类比较智能体之间的可能轨迹，使用这些数据学习奖励函数；
方法降低了人类监督的成本，从而可实际应用到最先进的RL系统；

## 实验和结论

方法可以有效的解决复杂的RL任务，而无需访问奖励函数；
证明了可以使用大约一个小时的人类时间成功地训练复杂的新行为；

## 自己思考

其实将评价每个结果变为了评价整体的轨迹段，这样子的好处是人类更容易评估；
人类评估的奖励由严格设置变为一个映射，判断该轨迹段是否适合人类；

# 005

英语：Automatic Detection of Machine Generated Text: A Critical Survey
中文：机器生成文本自动检测：一个关键的综述
2020

## 问题和现状

TGM（Text Generative model）擅长生成与人类语言风格相匹配的文本，并容易被对手滥用；
将TGM生成的文本与人类书写的文本区分开来日益重要；

## 自己思考

本质上还是文本分类任务，除了统计特征和利用PLM没有什么崭新的，这可能是由于2020年对此技术还不需要过多关注的原因

# 006

英语：DetectGPT: Zero-Shot Machine-Generated Text Detection using Probability Curvature
中文：DetectGPT: 使用概率曲率的零样本机器生成文本检测
2023

## 问题和现状

LLM的发展增加了对应检测系统的需求；

## 创新和方法

证明了从LLM中采样的文本倾向于占据模型的对数概率函数的负曲率区域；
定义了一个新的基于曲率的标准DetectGPT,来判断是否一段话由LLM生成；
该方法不需要单独训练分类器，不需要收集数据集，不需要显式地为文本添加水印；
只用有兴趣的模型计算似然概率和另一个生成模型的文章的随机扰动（probabilities perturbations）；
假设：模型生成文本在轻微重写下log概率往往低于原始样本，人类书写文本的轻微重写可能高于或低于原始log概率；即模型生成的文本往往位于对数概率函数具有负曲率的区域；

## 实验和结论

比现在的零样本检测方法更具有鉴别性，在AUROC有所超越；
在XSum、SQuAD、Reddit WritingPrompts，WMT16，PubMedQA 上进行测试；
使用AUROC作为评价指标

## 自己思考

AUROC可以理解为分类器正确地将随机选择的正（机器生成）示例排序高于随机选择的负（人类编写）示例的概率，此时所有的实验都用等量的正例和反例；
通过添加扰动可以发现是否由指定LM生成的文本，但是这只针对不同model，针对现在的ChatGPT不适用，因为ChatGPT还未能开源，且模型巨大，难以复现；

# 007

英语：TweepFake: About detecting deepfake tweets
中文：检测深度造假推文
2023

## 问题和现状
LLM的不断发展，伪造信息不断出现；
机器生成的文本在实际的社交环境中很难被监测；
缺少一个对应的数据集；
三类方法：1，从0开始的分类器；2.zero-shot，用PLM的输出作为分类器特征。3.微调LLM，在后面添加简单的层
## 创新和方法

构建数据集TweepFake，从23个机器人那里收集推文，模仿17个人类账户；

## 实验和结论
评估了13种不同的深度伪造文本检测方法；

## 自己思考
文章的relate work中对于最近的检测工作总结的很详细；
进一步验证了LM的飞速发展导致检测方法逐渐过时，需要进一步更新；
本质上缺乏检测技术的更进一步，仍是换PLM；

# 008
英语：Deepfake Text Detection: Limitations and Opportunities
中文：深度造假文本检测：局限和机遇
2023
## 问题和现状
LM创建令人信服的文本；
滥用伪造文本会误导人们；
已经提出了若干针对Deepfake Text Detection的方式；
缺乏外界环境的真实数据集？（TweepFake: About detecting deepfake tweets）其实并不缺乏;
通过制作对抗样本来

## 创新和方法
通过基于transformer的4个在线工具生成deepfake text，收集的数据用于评估检测手段的防御能力；
评估了6种最先进的防御方法的性能；
开发了几种低成本的对抗性攻击，研究了现有防御对自适应攻击者的鲁棒性；
提出并评估一种黑盒对抗样本制作策略 DFTFooler，无需了解防御模型，使用一个公开的PLM来制造对抗性的扰动；可产生可转移的对抗性样本，降低多重防御的性能；
## 实验和结论
发现多种防御手段在现有的评估场景下表现出明显的性能下降；
挖掘文本内容中的语义信息是提高深度伪造文本检测方案的鲁棒性和泛化性的一种有前景的方式。
## 自己思考
与常规方案不同，FAST的图形学方法可以多了解一下；
GRUEN，衡量文本质量，越高则越接近人类语言；
文章对FAST进行了详细的分析，注意看完FAST后，需要分析一下DistilFAST；


## 生成模型
**生成**
LM是统计模型，提供n个token的序列分布$x_1, ..., x_n$；
$p(x_{0},\cdot\cdot\cdot\ ,x_{n})=\prod_{t=0}^{n}p(x_{t+1}\mid x_{0},\cdot\cdot\cdot,x_{t})$;
给定初始启动序列输入LM，不断生成下一个token的条件概率$p(x_1|x_0)$
**解码**
top-k采样，核采样，温度采样，贪心采样；
**当前任务**
了解现有防御检测合成文本的现实适用性:
&emsp;1.理解并提高在自然环境中的防御性能；
&emsp;2.理解并改进针对自适应攻击者的性能；

## 论文创新
### 文本检测方案
利用LM提取特征构建二分类器；
采用的有：GROVER;GLTR-BERT;GLTR-GPT2;BERT-Defense;FAST;RoBERTa-Defense;
性能指标：
    BERT-Defense: accuracy; AUC;
    GLTR: AUC;
    FAST/GROVER: paired accuracy；
    本文：F1；
### 衡量文本检测的防御性能
当文本检测器面对自然文本时，防御的效果；
1.检测性能变化百分比：$\Delta F1$, $\Delta Recall$
$\Delta F1\,\,=\,\,(F1_{n e w}\,-\,F1_{b a s e l i n e})/(F1_{b a s e l i n e})$
2.自适应攻击者通过扰乱生成样本来逃避检测。这种情况下，使用逃避率，定义为被干扰的合成样本逃避防御检测的比例。Evasion rate；
3.GRUEN来衡量自适应攻击者（LM）生成的语言质量。
### 自然文本数据集
统计了若干未确定LM的自然数据集；
AI-Writer; ArticleForge；Kafkai；RedditBot; 
针对它们生成的文本质量，与研究界自己生成的文本质量相当（GRUEN下）；
### 攻击手段
1.不重新训练LM下改变文本生成过程（解码方式）；
2.为生成文本添加额外的扰动；
**DFTFooler**
添加扰动实现；只需要一个PLM；
1.给定一个文章，选择前N个PLM预测最为自信的单词（概率最大），进行替换；
2.找到替代词，并保留语义：
    1.同义词提取：为选择的目标词选择一组同义词；使用Counter-fitting Word Vectors to Linguistic Constraints论文中模型的embedding之间的余弦相似度；
    2.词性检查：确保同义词有相同词性；
    3.语义相似度检查：确保替换词前后的语义相似，使用Universal Sentence Encoder进行句子编码；
    4.选择LM测量的置信度较低的同义词；
## 发现
1.开放域的防御手段在面临自然界的合成文本时，体现出显著的性能下降；使用特定域训练的防御手段可以检测对应域的自然数据。
2.充足预训练的双向PLM，泛化效果优于单向模型；
3.使用10个自然数据来微调模型可以辅助防御手段适应新的域，数据越多越好；
4.改变解码策略可以攻破大多防御手段；
5.改变解码策略攻破的防御手段很可能是依赖于检测LM的token的差异的分类器；
6.条件生成文本上训练的防御手段不能检测无条件生成的文本；
7.DFTFooler允许在没有任何防御手段的相关信息下成功生成对抗样本；
8.DFTFooler使用双向LM时，将提供更有效的对抗样本；
9.增加扰动单词的数量可以提高攻击成功率，但降低文本质量；
10捕捉文本事实结构的语义特征，即实体级特征，提供了对自适应攻击的鲁棒性和更好的泛化性能；


# 009
英语：
中文：
2023

## 问题和现状

## 创新和方法

## 实验和结论

## 自己思考
