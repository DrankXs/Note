# 001*
英文：Efficiently Summarizing Text and Graph Encodings of Multi-Document Clusters
中文：针对多文档集群有效的摘要总结和图编码
2022

## 问题与现状
使用Transformer的大规模预训练模型面对多文档等长输入时，由于编码长度限制和二次内存增长，是很难应用的。
且它们不能很好的应用语义图。

## 创新与方法
提出了基于Encoder和Decoder Transformer模型的**高效图形化多文档摘要**方法；
包含了一种**有效的编码机制**，避免了传统Transformer的二次内存增长。
这种组合适用于创建的大规模输入文档，也能够处理辅助图（从多文档集群推导出的）格式的额外输入。
提出的机制，能将图信息合并到仅在文本上预训练的Encoder-Decoder模型中。
双重编码机制，一部分编码常规文本，一部分编码图形文本。

## 实验与结论
对Multi-News数据集进行了显著改进，总体上比以前的工作平均提高了1.8个ROUGE分数(Li等人，2020年)。我们还展示了ducc -2004数据集上仅tranfer的改进。
图形的输入将导致摘要更加抽象，人类评价表示这样的摘要将更符合原文。

## 自己思考
1.对BART（seq2seq）进行LongFormer方式的改造，包含（1.整体文本局部注意力；2.部分token全局注意力）
2.对于图的构建：句子级的三元组，每个句子形成主谓宾，并用共指关系等方式合并节点，对于摘要这是很好的一个方法。
3.采用将图线性化的序列表示图，这接近早先使用图学习的方法如随机游走，多跳随机游走等，那是不是可以改造为图神经网络来替代文中所述的图Encoder；
4.BART，S2S的预训练模型，不如换T5试试，使用提示学习且知识更加丰富。

# 002*
英文：Efficient Transformers: A Survey
中文：高效Transformer：综述
2022

## 问题与现状
Transformer架构极其有效，NLP中已经不可或缺；
变种former大多围绕计算和内存效率改进。

**self-attention**
可以看做图的归纳偏差，问题在于二次时间和内存复杂度。
为解决这类问题的Transformer称为**Efficient Transformers**

## 自己思考
提升Transformer效率的方式有很多，但在我看来，总得方法即global token+local attention的设置。其中global token的含义就是用有限个token来捕捉全局注意力，而local attention即限制每个token捕捉注意力的范围。
随着Transformer的研究，其实主要集中在如何构建local attention的范围，简单的诸如滑动窗口，切块的方法，也有人尝试随机选取，以及聚类等方式。
也有尝试将attention矩阵进行降秩的操作，从而直接减少复杂度，不过这个需要对线代知识充分熟悉。

## Tranformer架构
![20230215085408](https://cdn.jsdelivr.net/gh/DrankXs/picrepo/20230215085408.png)
具体可看BERT相关代码，或者多个解释。
1.Embedding + PositionEmbedding
2.MultiHeadSelfAttention
3.残差连接+正则化
4.双层线性层，激活函数ReLU
5.残差连接+正则化


# 003
知识图谱嵌入技术研究综述
2022

## 问题与现状
知识图谱：利用图模型来描述知识和建模事物之间关联关系的技术。
知识图谱嵌入（KGE）将知识图谱中的实体和关系embedding到连续的向量空间中。
将KG中事实embedding；添加时间维度的动态KG Embedding方式
![20230218093618](https://cdn.jsdelivr.net/gh/DrankXs/picrepo/20230218093618.png)

## 基于距离的模型
$\mathbf{h+r}\approx\mathbf{t}$ 向量，头结点向量+关系向量=尾节点向量

**SE Structured embedding**
计算实体在关系的对应空间中的投影向量之间的距离。
每个关系存在两个投影矩阵，一个用于投影头结点，一个用于尾节点。
$f_{r}\left(h,t\right)=-\left|\left|\mathbf{M}_{r,1}h-\mathbf{M}_{r,2}t\right|\right|$
表明头实体和尾实体在关系r下的语义相关度。

**翻译模型**
TransE
关系 r 的向量 r 被解释为头实体向量 h 与尾实体向量 t 之间的平移. 
嵌入的实体 h 和 t 可以通过平移向量 r 以低误差连接。
$\mathbf{h+r}\approx t$
$f_{r}(h,t)=-\left\Vert\mathbf{h}+\mathbf{f}-\mathbf{t}\right\Vert_{1/2}$
即向量$h+r$与t的$L_1$或$L_2$距离。
UM Unstructured model:
非结构模型，将知识图谱视为单关系图（没有r点），UM的评分函数为：$f_{r}(h,t)=-\|\mathbf{h}-\mathbf{t}\|_{2}^{2}$

**复杂关系建模**
Point-Wise空间---TransH:
实体为向量，关系r为法向量，对于三元组$(h,r,t)$，先将头实体和尾实体投影到关系r的法向量上对应的超平面上。
${\bf h}_{\perp}={\bf h}-{\bf w}_{r}^{\top}{\bf h}{\bf w}_{r}\;\;{\bf t}_{\perp}={\bf t}-{\bf w}_{r}^{\top}{\bf t}{\bf w}_{r}$
若三元组成立，
$\mathbf{h}_{\perp}+\mathbf{r}\approx\mathbf{t}_{\perp}$
评分函数为：$f_{r}(h,t)=-\|\mathbf{h}_{\perp}+\mathbf{r}-\mathbf{t}_{\perp}\|_{2}^{2}$
Point-Wise空间---TransR:
对于每个关系r，设置对应的投影矩阵$\mathbf{M}_{r}\in\mathbb{R}^{k\times d}$，利用投影矩阵投影实体，最后的评分函数是：$f_{r}(h,t)=-\|\mathrm{h}_{\perp}+r-\mathrm{t_{\perp}}\|_{2}^{2}$








